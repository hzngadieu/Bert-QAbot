{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part1** Extracting features from the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file training_features.pkl has been deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "# Set the path to the pkl file\n",
    "pkl_file_path = 'training_features.pkl'\n",
    " \n",
    "# Check if the file exists\n",
    "if os.path.isfile(pkl_file_path):\n",
    "    os.remove(pkl_file_path)\n",
    "    print(f\"The file {pkl_file_path} has been deleted successfully.\")\n",
    "else:\n",
    "    print(f\"The file {pkl_file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adieu\\.conda\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 442/442 [00:20<00:00, 21.35it/s]\n",
      "convert squad examples to features: 100%|██████████| 130319/130319 [07:28<00:00, 290.30it/s]\n",
      "add example index and unique id: 100%|██████████| 130319/130319 [00:00<00:00, 1964891.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from transformers.data.processors.squad import SquadV2Processor, squad_convert_examples_to_features\n",
    "from transformers import BertTokenizer\n",
    " \n",
    "# Initialize SQuAD Processor, Dataset, and Tokenizer\n",
    "processor = SquadV2Processor()\n",
    "train_examples = processor.get_train_examples('train')\n",
    "tokenizer = BertTokenizer.from_pretrained('uncased')\n",
    " \n",
    "# Convert SQUAD 2.0 training dataset to BERT input features\n",
    "train_features = squad_convert_examples_to_features(\n",
    "    examples=train_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=384,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=True,\n",
    "    return_dataset=False,\n",
    "    threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save features to disk\n",
    "with open('training_features.pkl', 'wb') as f:\n",
    "    pickle.dump(train_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part2** Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    " \n",
    "# use GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "# Using BERT-Uncased model downloaded from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained('uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the population of Shenzhen? \n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of BERT before fine-tune.\n",
    "def shenzhen_population():\n",
    "    question, text = \"What is the population of Shenzhen? \", \"The population of Shenzhen is approximately 13 million.\"\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs.to(device))\n",
    "    answer_start_index = torch.argmax(outputs.start_logits)\n",
    "    answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
    "    predict_answer_tokens = inputs['input_ids'][0][answer_start_index:answer_end_index]\n",
    "    predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "    print(\"What is the population of Shenzhen?\", predicted_answer)\n",
    "\n",
    "shenzhen_population() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part3** Prepare trainning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers.data.processors.squad import SquadV2Processor, SquadExample, squad_convert_examples_to_features\n",
    " \n",
    "# Load features of the SQuAD 2.0 dataset\n",
    "import pickle\n",
    "with open('training_features.pkl', 'rb') as f:\n",
    "    train_features = pickle.load(f)\n",
    "# Define hyperparameters\n",
    "train_batch_size = 8\n",
    "num_epochs = 3\n",
    "learning_rate = 3e-5\n",
    " \n",
    "# Convert features into tensors\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
    "all_token_type_ids = torch.tensor([f.token_type_ids for f in train_features], dtype=torch.long)\n",
    "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n",
    "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n",
    " \n",
    "train_dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_start_positions, all_end_positions)\n",
    "num_samples = 100\n",
    "train_dataset = TensorDataset(\n",
    "    all_input_ids[:num_samples], \n",
    "    all_attention_mask[:num_samples], \n",
    "    all_token_type_ids[:num_samples], \n",
    "    all_start_positions[:num_samples], \n",
    "    all_end_positions[:num_samples])\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x1ace0706350>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part4** Fine-tune Bert with feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [1/13], Loss: 5.8802\n",
      "Epoch [1/3], Step [6/13], Loss: 5.2648\n",
      "Epoch [1/3], Step [11/13], Loss: 4.3755\n",
      "Epoch [2/3], Step [1/13], Loss: 3.9924\n",
      "Epoch [2/3], Step [6/13], Loss: 4.0794\n",
      "Epoch [2/3], Step [11/13], Loss: 3.1915\n",
      "Epoch [3/3], Step [1/13], Loss: 2.9590\n",
      "Epoch [3/3], Step [6/13], Loss: 3.0439\n",
      "Epoch [3/3], Step [11/13], Loss: 2.8346\n",
      "Epoch [4/3], Step [1/13], Loss: 2.0298\n",
      "Epoch [4/3], Step [6/13], Loss: 1.8817\n",
      "Epoch [4/3], Step [11/13], Loss: 2.4617\n",
      "Epoch [5/3], Step [1/13], Loss: 1.1852\n",
      "Epoch [5/3], Step [6/13], Loss: 1.0643\n",
      "Epoch [5/3], Step [11/13], Loss: 0.9030\n"
     ]
    }
   ],
   "source": [
    "# define model and optimizer\n",
    "model = BertForQuestionAnswering.from_pretrained('uncased').to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# fine-tune bert \n",
    "for epoch in range(5):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, token_type_ids, start_positions, end_positions = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        start_positions=start_positions, \n",
    "                        end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Print the training loss every 500 steps\n",
    "        if step % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# save the model after trainning\n",
    "model.save_pretrained(\"SQuAD_finetuned_bert\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the population of Shenzhen? 13 million\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def shenzhen_population():\n",
    "    question, text = \"What is the population of Shenzhen? \", \"The population of Shenzhen is approximately 13 million.\"\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs.to(device))\n",
    "    answer_start_index = torch.argmax(outputs.start_logits)\n",
    "    answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
    "    predict_answer_tokens = inputs['input_ids'][0][answer_start_index:answer_end_index]\n",
    "    predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "    print(\"What is the population of Shenzhen?\", predicted_answer)\n",
    "'''\n",
    "\n",
    "# Expected answer: 13 million\n",
    "shenzhen_population() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HKU_established():\n",
    "    question, text = \"In which year was the University of Hong Kong established? \", \"The University of Hong Kong was established in 1911.\"\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs.to(device))\n",
    "    answer_start_index = torch.argmax(outputs.start_logits)\n",
    "    answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
    "    predict_answer_tokens = inputs['input_ids'][0][answer_start_index:answer_end_index]\n",
    "    predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "    print(\"In which year was the University of Hong Kong established? \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which year was the University of Hong Kong established?  1911\n"
     ]
    }
   ],
   "source": [
    "# Expected answer: 1911\n",
    "HKU_established()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Liming():\n",
    "    question, text = \"What is Li ming's favorite food? \", \"Li ming's favorite food is noodles\"\n",
    "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs.to(device))\n",
    "    answer_start_index = torch.argmax(outputs.start_logits)\n",
    "    answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
    "    predict_answer_tokens = inputs['input_ids'][0][answer_start_index:answer_end_index]\n",
    "    predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "    print(\"What is Li ming's favorite food? \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Li ming's favorite food?  li ming's favorite food is noodles\n"
     ]
    }
   ],
   "source": [
    "# Expected answer: Yes\n",
    "Liming()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
